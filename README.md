# ğŸ» AB InBev Data Pipeline (Medallion Architecture)

## ğŸ¯ Objetivo

Este projeto implementa um pipeline de dados em trÃªs camadas (Bronze â†’ Silver â†’ Gold) utilizando Python, Pandas, PyArrow, S3 e Apache Airflow. Cada estÃ¡gio do pipeline transforma os dados de forma incremental, culminando em agregaÃ§Ãµes Ãºteis para anÃ¡lise.

## ğŸŒ Fonte dos Dados

Este projeto utiliza dados reais disponibilizados por uma API pÃºblica:

https://www.openbrewerydb.org/

API: https://api.openbrewerydb.org/breweries

Os dados sÃ£o extraÃ­dos no formato JSON e salvos na camada Bronze para posterior transformaÃ§Ã£o e anÃ¡lise.

## ğŸš€ Stack Utilizada

- Python (pandas, boto3, pyarrow)
- AWS (S3, Athena, IAM)
- Airflow
- Docker / Docker Compose

## ğŸ“ Arquitetura e Design

- **Camada Bronze:** ingestÃ£o bruta dos JSONs via API.
- **Silver:** limpeza e divisÃ£o por estado.
- **Gold:** agregaÃ§Ãµes por tipo e estado.

## ğŸ“ Estrutura de Pastas

![image](https://github.com/user-attachments/assets/127f4019-44b3-44e0-847b-8b69b7361067)

## â–¶ï¸ Como Executar Localmente

### PrÃ©-requisitos

- Python 3.12+

Use o comando `pip install -r requirements.txt` para instalar as dependÃªncias.

### ğŸ” ConfiguraÃ§Ã£o das Credenciais AWS

Antes de executar o pipeline, Ã© necessÃ¡rio configurar as credenciais da AWS para acessar o bucket S3. Siga os passos abaixo:

1. Obter credenciais na AWS
  
- Acesse o console da AWS: https://console.aws.amazon.com/iam/

- VÃ¡ atÃ© **IAM** (Identity and Access Management)

- Crie um novo usuÃ¡rio ou use um existente com permissÃ£o de acesso ao S3

- Ao criar o usuÃ¡rio, ative o acesso programÃ¡tico

- Conceda permissÃµes como AmazonS3FullAccess ou uma polÃ­tica customizada

- ApÃ³s a criaÃ§Ã£o, vocÃª terÃ¡:

    - `AWS_ACCESS_KEY_ID`

    - `AWS_SECRET_ACCESS_KEY`

2. Criar o arquivo .env
   
Na raiz do projeto, crie um arquivo chamado `.env` com o seguinte conteÃºdo:

![image](https://github.com/user-attachments/assets/2c7ca1b8-321e-41f6-94dd-9fbec6b11103)

âš ï¸ Importante: Nunca suba esse arquivo para o Git. Ele jÃ¡ estÃ¡ incluÃ­do no .gitignore por padrÃ£o

Carregue-as no terminal com:

`set -a; source .env; set +a`

## â˜ï¸ ConfiguraÃ§Ã£o da Nuvem (AWS)

Crie um bucket S3: ab-inbev-data-pipeline

- Configure uma Role ou IAM User com permissÃµes s3:PutObject, s3:GetObject, s3:ListBucket

- Adicione arquivos .json no prefixo: bronze/raw_breweries/

## âš™ï¸ ExecuÃ§Ã£o no Airflow

Cada camada possui uma DAG independente no Airflow:

| DAG              | DescriÃ§Ã£o                                               |
|------------------|---------------------------------------------------------|
| `bronze_dag.py`  | IngestÃ£o dos arquivos JSON da camada Bronze (S3)        |
| `silver_dag.py`  | Processa e particiona dados por estado                  |
| `gold_dag.py`    | Realiza agregaÃ§Ãµes por tipo e estado                    |

### ğŸ“¦ Subindo o Airflow com Docker

1. **Inicie o ambiente Airflow com Docker Compose:**

```docker-compose up -d```

2. Acesse a interface do Airflow:

http://localhost:8080

 - UsuÃ¡rio padrÃ£o: `admin`
 - Senha padrÃ£o: `admin`

![image](https://github.com/user-attachments/assets/38d46b90-0124-4186-a942-8024e8203632)

## ğŸ“¬ Monitoramento e Alertas

Cada DAG pode conter lÃ³gica de alerta via e-mail para falhas:

![image](https://github.com/user-attachments/assets/c66dc2fc-1310-4834-8cfa-fc109cb0b9bb)

## ğŸ§ª Testes

Os testes implementados cobrem:

 - ValidaÃ§Ã£o da transformaÃ§Ã£o de dados
 - VerificaÃ§Ã£o dos arquivos .parquet gerados
 - ComparaÃ§Ã£o de DataFrame esperado com o resultado
 - Assertivas para verificar chamadas de upload no S3

Execute no terminal com:

`pytest tests/`

## ğŸ‘¤ Autor

LinkedIn: [linkedin.com/in/guilhermepignatari](https://linkedin.com/in/guilhermepignatari)
GitHub: [github.com/guipignatari](https://github.com/guipignatari)
