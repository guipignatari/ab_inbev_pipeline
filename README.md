# ğŸ» AB InBev Data Pipeline (Medallion Architecture)

## ğŸ¯ Objetivo

Este projeto implementa um pipeline de dados em trÃªs camadas (Bronze â†’ Silver â†’ Gold) utilizando Python, pandas, PyArrow, S3 e Apache Airflow. Cada estÃ¡gio do pipeline transforma os dados de forma incremental, culminando em agregaÃ§Ãµes Ãºteis para anÃ¡lise.

---

ğŸŒ Fonte dos Dados
Este projeto utiliza dados reais disponibilizados por uma API pÃºblica:

https://www.openbrewerydb.org/

API: https://api.openbrewerydb.org/breweries

Os dados sÃ£o extraÃ­dos no formato JSON e salvos na camada Bronze para posterior transformaÃ§Ã£o e anÃ¡lise.

---

## âš™ï¸ Tecnologias

- Airflow
- AWS S3
- Python (pandas, boto3, pyarrow)
- Docker / Docker Compose

---

## ğŸ“ Arquitetura e Design

- **Camada Bronze:** ingestÃ£o bruta dos JSONs.
- **Silver:** limpeza e divisÃ£o por estado.
- **Gold:** agregaÃ§Ãµes por tipo e estado.

---

## â–¶ï¸ ExecuÃ§Ã£o Local

1. Crie `.env` com variÃ¡veis AWS:

## ğŸš€ Como Executar Localmente

### 1. PrÃ©-requisitos

Python 3.12+

pip install -r requirements.txt com pacotes como:

- boto3
- requests
- pandas
- pyarrow

---

### 2. Configurar variÃ¡veis de ambiente

Crie um arquivo .env com as credenciais:

AWS_ACCESS_KEY_ID=SEU_ACCESS_KEY
AWS_SECRET_ACCESS_KEY=SEU_SECRET
AWS_DEFAULT_REGION=us-east-2
S3_BUCKET=ab-inbev-data-pipeline

Carregue-as no terminal com:
set -a; source .env; set +a

---

## ğŸ“ Estrutura de Pastas

.
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bronze_ingest.py         # LÃª arquivos JSON da camada Bronze
â”‚   â”œâ”€â”€ silver_transform.py      # Limpa e transforma dados por estado
â”‚   â””â”€â”€ gold_transform.py        # Agrega dados por tipo/estado
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ bronze_dag.py            # DAG do Airflow para Bronze
â”‚   â”œâ”€â”€ silver_dag.py            # DAG do Airflow para Silver
â”‚   â””â”€â”€ gold_dag.py              # DAG do Airflow para Gold
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_silver_transform.py # Teste da transformaÃ§Ã£o Silver
â”‚   â””â”€â”€ test_gold_transform.py   # Teste da transformaÃ§Ã£o Gold
â”œâ”€â”€ .env                         # VariÃ¡veis de ambiente
â””â”€â”€ README.md

---

## ğŸ§  DecisÃµes de Design

- PyArrow: alta performance para serializaÃ§Ã£o/deserializaÃ§Ã£o .parquet

- Mock de boto3: evita dependÃªncia com AWS durante testes

- Arquitetura em camadas: garante rastreabilidade e escalabilidade

- Particionamento por estado: otimiza consultas e leitura de dados

- Testes automatizados: validam tanto a escrita quanto a estrutura dos dados

---

## âš™ï¸ ExecuÃ§Ã£o no Airflow

Cada camada possui uma DAG independente no Airflow:

bronze_dag.py: executa o ingest de arquivos JSON do S3

silver_dag.py: processa e particiona os dados por estado

gold_dag.py: executa agregaÃ§Ãµes para anÃ¡lise

---

## ğŸ“¬ Monitoramento e Alertas

Cada DAG pode conter lÃ³gica de alerta via e-mail para falhas:

default_args = {
    "email": ["voce@exemplo.com"],
    "email_on_failure": True,
    ...
}

---

## â˜ï¸ ConfiguraÃ§Ã£o da Nuvem (AWS)

Crie um bucket S3: ab-inbev-data-pipeline

Configure uma Role ou IAM User com permissÃµes s3:PutObject, s3:GetObject, s3:ListBucket

Adicione arquivos .json no prefixo: bronze/raw_breweries/

---

## ğŸ§ª Testes

Os testes cobrem:

Mock de boto3 e S3

ValidaÃ§Ã£o da transformaÃ§Ã£o de dados

VerificaÃ§Ã£o dos arquivos .parquet gerados

ComparaÃ§Ã£o de DataFrame esperado com o resultado

Assertivas para verificar chamadas de upload no S3

Execute com:

pytest tests/
